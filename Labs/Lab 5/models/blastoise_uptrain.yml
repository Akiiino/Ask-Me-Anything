model: AttentionSeq2Seq
model_params:
  vocab_source: prepared/words_vocab
  vocab_target: prepared/prons_vocab
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params:
    num_units: 256
  bridge.class: seq2seq.models.bridges.ZeroBridge
  embedding.dim: 75
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: GRUCell
      cell_params:
        num_units: 256
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 0.9
      num_layers: 2
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: GRUCell
      cell_params:
        num_units: 256
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 3
  optimizer.name: Adam
  optimizer.learning_rate: 0.00001
  source.max_seq_len: 15
  source.reverse: false
  target.max_seq_len: 15

buckets: 4, 7, 10, 15
hooks:
  - class: TrainSampleHook
    params:
      every_n_steps: 1000
      source_delimiter: ""
      target_delimiter: _
  - class: TokensPerSecondCounter
    params:
       every_n_steps: 200
input_pipeline_train:
  class: ParallelTextInputPipeline
  params:
    source_files:
      - prepared/train_words
    target_files:
      - prepared/train_pronunciations
input_pipeline_dev:
  class: ParallelTextInputPipeline
  params:
    source_files:
      - prepared/validation_words
    target_files:
      - prepared/validation_pronunciations
batch_size: 256
tf_random_seed: 1337
train_steps: 20000
eval_every_n_steps: 200
save_checkpoints_steps: 500
keep_checkpoint_max: 2
keep_checkpoint_every_n_hours: 2
output_dir: models/blastoise
