{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 4. Word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Для начала, как всегда, всевозможные приготовления к работе. Использовать будем заранее предобработанные текты из трёх википедий — английской, русской и simple. Большая часть лабы выполнялась на full, но переключение на любую другую производится заменой единственного параметра (не то чтобы это было неочевидно, впрочем)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from itertools import accumulate\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import stop_words\n",
    "import regex as re\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "import sklearn.decomposition\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Wiki:\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.__set_consts()\n",
    "        \n",
    "        if not os.path.exists(wiki):\n",
    "            os.mkdir(wiki)\n",
    "        \n",
    "        self.counter = self.vocab = self.rev_vocab = None\n",
    "        self.counter, self.vocab, self.rev_vocab = self.__get_vocab()\n",
    "        \n",
    "\n",
    "    def __set_consts(self):\n",
    "        wikis = {\n",
    "            'full': {\n",
    "                'lang'      : 'en',\n",
    "                'wiki_file' : '/Users/akiiino/Documents/Wiki/full/processed.dat',\n",
    "                'vocab_size': 150000\n",
    "            },\n",
    "            'simple': {\n",
    "                'lang'      : 'en',\n",
    "                'wiki_file' : '/Users/akiiino/Documents/Wiki/simple/processed.dat',\n",
    "                'vocab_size': 50000\n",
    "            },\n",
    "            'ru': {\n",
    "                'lang'      : 'ru',\n",
    "                'wiki_file' : '/Users/akiiino/Documents/Wiki/ru/processed.dat',\n",
    "                'vocab_size': 100000\n",
    "            },\n",
    "        }\n",
    "\n",
    "        \n",
    "        self.LANG = wikis[self.wiki]['lang']\n",
    "        self.WIKI_FILE = wikis[self.wiki]['wiki_file']\n",
    "        self.VOCAB_SIZE = wikis[self.wiki]['vocab_size']\n",
    "        \n",
    "        self.WORD_COUNT_FILE = os.path.join(self.wiki, 'word_counts')\n",
    "        self.LSA_TFIDF_FILE = os.path.join(self.wiki, 'LSA_TfIdf.npz')\n",
    "        self.LSA_EMBED_FILE = os.path.join(self.wiki, 'LSA_embed.npy')\n",
    "        self.LSA_VOCAB_FILE = os.path.join(self.wiki, 'LSA_vocab')\n",
    "        self.W2V_EMBED_FILE = os.path.join(self.wiki, 'W2V_embed.npy')\n",
    "        self.GLOVE_COOCC_FILE = os.path.join(self.wiki, 'glove_coocc.npz')\n",
    "        self.GLOVE_EMBED_FILE = os.path.join(self.wiki, 'glove_embed.npy')\n",
    "        self.TSNE_FILE = os.path.join(self.wiki, 'tsne.npy')\n",
    "        \n",
    "        self.stop_words = set(stop_words.get_stop_words(self.LANG))\n",
    "        self.punct_replace = re.compile(r\"\\p{P}+\")\n",
    "\n",
    "\n",
    "    def _normalize(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.punct_replace.sub(\"\", text)\n",
    "        text = (word for word in text.split() if word not in self.stop_words)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def wiki_gen(self, use_vocab=True, chunk_size=2**26, single_pass=False, joined=False):\n",
    "        read = 0\n",
    "\n",
    "        with open(self.WIKI_FILE) as wiki_file:\n",
    "            while True:\n",
    "                lines = \" \".join(wiki_file.readlines(chunk_size))\n",
    "                read += len(lines)\n",
    "\n",
    "                if chunk_size >= 2**20:\n",
    "                    print('Pulling... total {} MB\\r'.format(read//2**20), end=\"\")\n",
    "\n",
    "                if not lines:\n",
    "                    if single_pass:\n",
    "                        return\n",
    "                    wiki_file.seek(0)\n",
    "                    print(\"Starting over...          \")\n",
    "                    continue\n",
    "\n",
    "                lines = self._normalize(lines)\n",
    "\n",
    "                if self.vocab is not None and use_vocab:\n",
    "                    lines = (self.vocab[word] if word in self.vocab else 0 for word in lines)\n",
    "\n",
    "                if not joined:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
    "                    yield lines\n",
    "                else:\n",
    "                    yield \" \".join(lines)\n",
    "        \n",
    "    def __get_vocab(self):\n",
    "        if not os.path.exists(self.WORD_COUNT_FILE):\n",
    "            counter = collections.Counter()\n",
    "\n",
    "            for chunk in self.wiki_gen(use_vocab=False, single_pass=True):\n",
    "                counter.update(chunk)\n",
    "\n",
    "            with open(self.WORD_COUNT_FILE, 'wb') as file:\n",
    "                pickle.dump(counter, file)\n",
    "\n",
    "        else:\n",
    "            with open(self.WORD_COUNT_FILE, 'rb') as file:\n",
    "                counter = collections.Counter(pickle.load(file))\n",
    "\n",
    "\n",
    "        unk_count = sum(counter.values()) - sum(x[1] for x in counter.most_common(self.VOCAB_SIZE - 1))\n",
    "        counter = dict([('UNK', unk_count)] + counter.most_common(self.VOCAB_SIZE - 1))\n",
    "        \n",
    "        vocab     = {word: num for num, (word, _) in enumerate(counter.items())}\n",
    "        rev_vocab = list(vocab.keys())\n",
    "        \n",
    "        return counter, vocab, rev_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wiki = Wiki(\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### **Задание**\n",
    "**1**.  Вычислите $X$ понравившимся способом (возможно, стоит использовать разреженную матрицу).\n",
    "\n",
    "**2**. Обучите представления слов при $k = 128$ на своем корпусе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Обучение устроено просто: пайплайн из `sklearn`'ового `TfidfVectorizer` для построения самой матрицы $X$, после чего Truncated SVD в виде `scipy.sparse.linalg.svds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_lsa(wiki):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=wiki.vocab)\n",
    "    tf_idf_matrix = vectorizer.fit_transform(\n",
    "        wiki.wiki_gen(\n",
    "            chunk_size=1,\n",
    "            use_vocab=False,\n",
    "            single_pass=True,\n",
    "            joined=True\n",
    "        )\n",
    "    ).T\n",
    "    np.savez(wiki.LSA_TFIDF_FILE, tf_idf_matrix)\n",
    "\n",
    "    print(\"Tf-Idf done\")\n",
    "    \n",
    "    svd = sklearn.decomposition.TruncatedSVD(n_components=128)\n",
    "    \n",
    "    lsa_embeds = svd.fit_transform(tf_idf_matrix)\n",
    "    print(\"SVD done\")\n",
    "    \n",
    "    return lsa_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_lsa(wiki):\n",
    "    if not os.path.exists(wiki.LSA_EMBED_FILE):\n",
    "        lsa_embeds = build_lsa(wiki)\n",
    "        np.save(wiki.LSA_EMBED_FILE, lsa_embeds)\n",
    "    else:\n",
    "        lsa_embeds = np.load(wiki.LSA_EMBED_FILE)\n",
    "        \n",
    "    return lsa_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lsa_embeds = get_lsa(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "При этом после вычисления всё сохраняется на диск, чтобы лишний раз не считать, если понадобится снова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**3**. Реализуйте поиск k-ближайших соседей для Евклидовой меры в 128-мерном пространстве ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Скелет для нахождения соседей: по вектору находим ближайшие слова; сложность линейная, т.к. использован `argpartition`. Сам вектор может быть получен как напрямую из слова, так и, например, сложениями и вычитаниями других векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def closest_to_vec(vec, embeds, wiki, metric_name='euclid', count=10, with_dists=False):\n",
    "        metrics = {\n",
    "            'euclid': euclidean_distances,\n",
    "            'cosine': cosine_similarity,\n",
    "            'manhattan': manhattan_distances\n",
    "        }\n",
    "        \n",
    "        metric = metrics[metric_name]\n",
    "        \n",
    "        dists = metric(\n",
    "                [vec],\n",
    "                embeds\n",
    "        )[0]\n",
    "        \n",
    "        if metric_name == \"cosine\":\n",
    "                closest_indexes = dists.argpartition(range(-count, 0))[-1:-count-1:-1]\n",
    "        else:\n",
    "                closest_indexes = dists.argpartition(range(0, count))[:count]\n",
    "\n",
    "        if with_dists:\n",
    "            return list(zip(dists[closest_indexes], [wiki.rev_vocab[x] for x in closest_indexes if x]))\n",
    "        else:\n",
    "            return [wiki.rev_vocab[x] for x in closest_indexes if x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Сам поиск по словам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def closest_words(word, embeds, wiki, metric='euclid', count=10, with_dists=False):\n",
    "    return closest_to_vec(embeds[wiki.vocab[word]], embeds, wiki, metric, count, with_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = [\"field\", \"directed\", \"financial\", \"road\", \"provides\", \"player\", \"2011\", \"edition\", \"battle\", \"ended\", \"son\", \"least\", \"mexico\", \"male\", \"medal\", \"big\", \"central\", \"according\", \"km\", \"year\", \"rights\", \"george\", \"founded\", \"tournament\", \"instead\", \"movie\", \"445\", \"system\", \"york\", \"win\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**4**. Найдите 10 ближайших представлений к 30 словам, которые выбрали сами. Попытайтесь сделать так, чтобы примеры были интересными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word.ljust(10), \":\", \" \".join(closest_words(word, lsa_embeds, wiki)[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Есть несколько интересных вещей. \n",
    "Например, следующее:\n",
    "- То, как сгруппированы числа (а особенно то, что рядом находятся числа схожей величины);\n",
    "- то, что рядом со словом \"york\" находятся также и \"jersey\", и \"new\", с которого обычно начинаются оба;\n",
    "- то, как \"central\" явно встречается преимущественно в географическом контексте;\n",
    "- Википедия явно очень беспокоится о правах в Латвии;\n",
    "- среди мужских имён почему-то затесалось \"wife\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**5**. Проделайте то же самое для косинусной меры. Какой результат вам показался более интересным?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word.ljust(10), \":\", \" \".join(closest_words(word, lsa_embeds, wiki, metric=\"cosine\")[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Тут вообще непонятно что происходит. Видимо, в длине векторов тоже есть важная информация и лучше было всё же с евклидовой метрикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**6**. Предложите свою меру длины, проверьте, как она работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word.ljust(10), \":\", \" \".join(closest_words(word, lsa_embeds, wiki, metric=\"manhattan\")[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В целом, примерно как и в евклидовой метрике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Сначала о теории:\n",
    "\n",
    "### **Задание**\n",
    "\n",
    "**1**. Как можно представить модель skip-gram в виде нейронной сети?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Тут практически нет отличий от CBOW. Два линейных слоя без нелинейности между ними, в конце --- softmax. На вход сеть принимает one-hot вектор размерности $W$, кодирующий центральное слово; на выходе --- вектор той же размерности, представляющий собой вероятности каждого слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**2**. Оцените сложность обучения skip-gram модели относительно параметров \n",
    "* $T$ - размер корпуса;\n",
    "* $W$ - размер словаря;\n",
    "* $c$ - радиус окна;\n",
    "* $d$ - размерность представлений;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Forward propagation:**\n",
    "\n",
    "$(d \\times W) \\cdot (W \\times 1); (W \\times d) \\cdot (d \\times 1) \\implies O(WD) + O(WD) = O(WD)$;\n",
    "\n",
    "Softmax --- ещё дополнительно линейное (от $W$) время. Итого $O(WD)$ --- ничего страшного.\n",
    "\n",
    "** Backpropagation:**\n",
    "    \n",
    "Взятие производных функции ошибки --- $O(Wdc)$, т.к. мы берём производные по матрицам размера $O(Wd)$, и наш размер контекста --- $c$. При этом нам надо это будет сделать $O(T)$ раз --- для каждого батча.\n",
    "\n",
    "Итого --- $O(TWdc)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Батчи генерируются из скользящего окна шириной `context_width`; из каждого окна берётся `context_examples` пар слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def w2v_batch_gen(batch_size, context_examples, context_width):\n",
    "    assert batch_size % context_examples == 0\n",
    "    assert context_examples <= 2 * context_width\n",
    "\n",
    "    gen = wiki.wiki_gen()\n",
    "    \n",
    "    pulled = 0\n",
    "    buffer = collections.deque()\n",
    "\n",
    "    while True:\n",
    "        while len(buffer) < 2 * context_width + batch_size // context_examples:\n",
    "            buffer.extend(next(gen))\n",
    "        \n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        span = 2 * context_width + 1\n",
    "\n",
    "        for i in range(batch_size // context_examples):\n",
    "            target = context_width\n",
    "            available_targets = list(range(span))\n",
    "            available_targets.remove(target)\n",
    "            for j in range(context_examples):\n",
    "                target = random.choice(available_targets)\n",
    "                available_targets.remove(target)\n",
    "                batch[i * context_examples + j] = buffer[context_width]\n",
    "                labels[i * context_examples + j, 0] = buffer[target]\n",
    "            buffer.popleft()\n",
    "        yield batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### **Задание**\n",
    "1. Выберите понравивщуюся вам функцию потерь. Оцените сложность обучения для нее, сравните с простым softmax.\n",
    "2. На основе оценки сложности определиться с количеством эпох, размера вектора $d$ = 256 будет достаточно. Определитесь с размером окна. Будьте готовы, что на большом корпусе обучение может длиться около суток."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Пользоваться будем NCE --- преимущественно из-за простоты реализации. Сложность обучения оценить легко: фактически, единственное существенное изменение (асимптотики, разумеется; сами алгоритмы отличаются существенно) заключается в том, что вместо прохода по всем $W$ словам, мы рассматриваем только $k$ cлучайных слов; итого сложность $O(Tcdk)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "embedding_dim = 256\n",
    "context_width = 6\n",
    "context_examples = 4\n",
    "neg_samples = 100\n",
    "\n",
    "w2v_graph = tf.Graph()\n",
    "with w2v_graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    all_embeddings = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            [wiki.VOCAB_SIZE, embedding_dim],\n",
    "            -1.0,\n",
    "            1.0\n",
    "        )\n",
    "    )\n",
    "    embed = tf.nn.embedding_lookup(all_embeddings, train_inputs)\n",
    "\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [wiki.VOCAB_SIZE, embedding_dim],\n",
    "            stddev=1.0 / math.sqrt(embedding_dim)\n",
    "        )\n",
    "    )\n",
    "    biases = tf.Variable(tf.zeros([wiki.VOCAB_SIZE]))\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=weights,\n",
    "            biases=biases,\n",
    "            labels=train_labels,\n",
    "            inputs=embed,\n",
    "            num_sampled=neg_samples,\n",
    "            num_classes=wiki.VOCAB_SIZE\n",
    "        )\n",
    "    )\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Экспериментальным путём выяснено, что десяти миллионов итераций достаточно, чтобы обучиться (по крайней мере до состояния, где заметных улучшений нет). Занимает это (у меня, по крайней мере) около восьми-девяти часов (надо бы, конечно, сделать две-три полные эпохи, но это очень уж долго. Может быть, потом). Девять часов --- это всё-таки немало, так что тут тоже есть возможность загружать и выгружать результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**3**. Обучите skip-gram модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_w2v(wiki, num_steps=500):\n",
    "    with tf.Session(graph=w2v_graph) as session:\n",
    "        init.run()\n",
    "\n",
    "        average_loss = 0\n",
    "        batch_gen = w2v_batch_gen(batch_size, context_examples, context_width)\n",
    "        for step in range(num_steps):\n",
    "            try:\n",
    "                for _ in range(10000):\n",
    "                    batch_inputs, batch_labels = next(batch_gen)\n",
    "                    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "                    _, loss_val= session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "                    average_loss += loss_val\n",
    "\n",
    "\n",
    "                clear_output()\n",
    "                print(\"Time: {}; steps: {}; avg. loss: {}.\".format(datetime.now().strftime(\"%H:%M:%S\"), step*10000, average_loss/10000))\n",
    "                average_loss = 0\n",
    "                w2v_embeds = all_embeddings.eval()\n",
    "                for i in range(10):\n",
    "                    word = wiki.rev_vocab[random.randint(1, 150)]\n",
    "                    print(word.ljust(10), \":\", \" \".join(closest_words(word, w2v_embeds, wiki)[1:]))\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "        w2v_embeds = all_embeddings.eval()\n",
    "\n",
    "    return w2v_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v(wiki):\n",
    "    if not os.path.exists(wiki.W2V_EMBED_FILE):\n",
    "        w2v_embeds = build_w2v(wiki)\n",
    "        np.save(wiki.W2V_EMBED_FILE, w2v_embeds)\n",
    "    else:\n",
    "        w2v_embeds = np.load(wiki.W2V_EMBED_FILE)\n",
    "        \n",
    "    return w2v_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_embeds = get_w2v(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**4**. Попробуйте снова найти ближайшие представления для тех 30 слов. Улучшился ли результат визуально? Попробуйте разные меры расстояния (евклидова, косинусная)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word.ljust(10), \":\", \" \".join(closest_words(word, w2v_embeds, wiki)[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Видно, что качество сильно улучшилось; вместо слов, которые относятся примерно к одной теме теперь рядом находятся слова, значащие примерно одно и то же; вместо всяких \"founded\" --- \"university\" и прочее, теперь тут \"founded: --- \"formed\", \"established\"... \"bought\"... В общем, теперь смысла гораздо больше (что было ясно ещё из сложений-вычитаний)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**5**. Найдите ближайшие вектора для выражения v(king) - v(man) + v(women). Если модель обучена хорошо, то среди ближайших векторов должно быть представление v(queen). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Функция для сложения-вычитания векторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def analogy(pos, neg, embeds, wiki, metric='euclid', count=10):\n",
    "    pos_vec = [embeds[wiki.vocab.get(w, 0)] for w in pos]\n",
    "    neg_vec = [embeds[wiki.vocab.get(w, 0)] for w in neg]\n",
    "\n",
    "    return [\n",
    "        pair for pair in closest_to_vec(\n",
    "            sum(pos_vec) - sum(neg_vec),\n",
    "            embeds,\n",
    "            wiki,\n",
    "            metric_name=metric,\n",
    "            count=count,\n",
    "            with_dists=True\n",
    "        ) if pair[1] not in pos and pair[1] not in neg\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Можно поэкспериментировать и посмотреть, что получится. Базовые примеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "analogy([\"king\", \"woman\"], [\"man\"], w2v_embeds, wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**6**. На основе арифметических операций над представлениями предложите алгоритмы, которые\n",
    "  * Для страны определяют столицу\n",
    "  * Для компании определяют CEO\n",
    "  * Для прилагательного определяют превосходную форму\n",
    "  * Для слова определяют множественное число\n",
    "  * Для слова находит его антоним"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#1. Столицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_capitals(countries):\n",
    "    return [\n",
    "        analogy(\n",
    "            [country,\n",
    "            \"moscow\",],\n",
    "            [\"russia\",],\n",
    "            w2v_embeds,\n",
    "            wiki\n",
    "        )[:2] for country in countries\n",
    "    ]\n",
    "\n",
    "countries = [\n",
    "    \"germany\",\n",
    "    \"france\",\n",
    "    \"japan\",\n",
    "    \"spain\",\n",
    "    \"canada\",\n",
    "    \"greece\",\n",
    "    \"albania\",\n",
    "    \"afghanistan\",\n",
    "    \"algeria\",\n",
    "    \"norway\",\n",
    "    \"india\"\n",
    "]\n",
    "print(\"\\n\".join(\": \".join((word.ljust(11), \", \".join(pair[1].rjust(10) for pair in closest))) for word, closest in zip(countries, get_capitals(countries))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Как видно, работает, но не всегда идеально. Возможно, стоит делать несколько предсказаний и усреднять, но у меня не получилось выжать из этого достойный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#2. CEO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_CEOs(companies):\n",
    "    return [\n",
    "        analogy(\n",
    "            [\"zuckerberg\", company],\n",
    "            [\"facebook\"],\n",
    "            w2v_embeds,\n",
    "            wiki\n",
    "        )[:2] for company in companies\n",
    "    ]\n",
    "\n",
    "companies = [\n",
    "    \"apple\",\n",
    "    \"sun\",\n",
    "    \"oracle\"\n",
    "]\n",
    "print(\"\\n\".join(\": \".join((word.ljust(11), \", \".join(pair[1].rjust(10) for pair in closest))) for word, closest in zip(companies, get_CEOs(companies))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Мда. Идея должна бы срaботать, но нужно нечеловеческое время на обучение и словарь побольше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#3. Прилагательные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_superlatives(adjectives):\n",
    "    return [\n",
    "        analogy(\n",
    "            [\"fastest\", adjective],\n",
    "            [\"fast\"],\n",
    "            w2v_embeds,\n",
    "            wiki\n",
    "        )[:2] for adjective in adjectives\n",
    "    ]\n",
    "\n",
    "adjectives = [\n",
    "    \"dark\",\n",
    "    \"big\",\n",
    "    \"good\",\n",
    "    \"sharp\",\n",
    "    \"strange\",\n",
    "    \"blue\",\n",
    "]\n",
    "print(\"\\n\".join(\": \".join((word.ljust(11), \", \".join(pair[1].rjust(10) for pair in closest))) for word, closest in zip(adjectives, get_superlatives(adjectives))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "О, работает! Почти... Смысл точно местами прослеживается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#4. Множественное число:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_plurals(nouns):\n",
    "    return [\n",
    "        analogy(\n",
    "            [\"boxes\", noun],\n",
    "            [\"box\"],\n",
    "            w2v_embeds,\n",
    "            wiki\n",
    "        )[:2] for noun in nouns\n",
    "    ]\n",
    "\n",
    "nouns = [\n",
    "    \"queen\",\n",
    "    \"moose\",\n",
    "    \"box\",\n",
    "    \"chair\",\n",
    "    \"glass\",\n",
    "]\n",
    "print(\"\\n\".join(\": \".join((word.ljust(11), \", \".join(pair[1].rjust(10) for pair in closest))) for word, closest in zip(nouns, get_plurals(nouns))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Не очень что-то."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Построим матрицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cooccur(wiki, context_width=5, chunk_size = 50000):\n",
    "    slices = list(\n",
    "        range(a, b) for a, b in zip(\n",
    "            range(0, wiki.VOCAB_SIZE + chunk_size, chunk_size),\n",
    "            range(chunk_size, wiki.VOCAB_SIZE + chunk_size, chunk_size)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mat = scipy.sparse.lil_matrix((wiki.VOCAB_SIZE, wiki.VOCAB_SIZE), dtype=np.float32)\n",
    "\n",
    "    for x, range_x in enumerate(slices):\n",
    "        for y, range_y in list(enumerate(slices))[x:]:\n",
    "            local_mat = np.zeros((chunk_size, chunk_size), dtype=np.float32)\n",
    "            print(\"starting chunk {}, {}\".format(x, y))\n",
    "\n",
    "            text_gen = wiki.wiki_gen(single_pass=True)\n",
    "\n",
    "            buffer = collections.deque()\n",
    "            pulled = 0\n",
    "\n",
    "            stop = False\n",
    "            i = 0\n",
    "            while True:\n",
    "                i += 1;\n",
    "                if i % 100000 == 0:\n",
    "                    print(\"{}\\r\".format(i), end=\"\")\n",
    "                while len(buffer) < context_width + 1:\n",
    "                    try:\n",
    "                        next_chunk = next(text_gen)\n",
    "                    except StopIteration:\n",
    "                        stop = True\n",
    "                        break\n",
    "                    buffer.extend(next_chunk)\n",
    "\n",
    "                if stop:\n",
    "                    break\n",
    "\n",
    "                for j in range(1, context_width+1):\n",
    "                    row, column = sorted([buffer[0], buffer[j]])\n",
    "                    if row in range_x and column in range_y:\n",
    "                        if x == y:\n",
    "                            local_mat[row - x*chunk_size, column - y*chunk_size] += 1./j\n",
    "                            local_mat[column - y*chunk_size, row - x*chunk_size] += 1./j\n",
    "                        else:\n",
    "                            local_mat[row - x*chunk_size, column - y*chunk_size] += 1./j\n",
    "                buffer.popleft()\n",
    "\n",
    "            mat[x*chunk_size:(x+1)*chunk_size, y*chunk_size:(y+1)*chunk_size] = local_mat\n",
    "            if x!= y:\n",
    "                mat[y*chunk_size:(y+1)*chunk_size, x*chunk_size:(x+1)*chunk_size] = local_mat.T\n",
    "    mat = scipy.sparse.lil_matrix(mat)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cooccur(wiki):\n",
    "    if not os.path.exists(wiki.GLOVE_COOCC_FILE):\n",
    "        cooccurrences_matrix = build_cooccur(wiki)\n",
    "        np.savez(wiki.GLOVE_COOCC_FILE, cooccurrences_matrix)\n",
    "        \n",
    "    else:\n",
    "        cooccurrences_matrix = np.load(wiki.GLOVE_COOCC_FILE).items()[0][1].item()\n",
    "\n",
    "    return cooccurrences_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cooccurrences_matrix = get_cooccur(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def glove_batch_gen(cooccur_mat, batch_size):\n",
    "    def single_gen():\n",
    "        for i, (row, data) in enumerate(zip(cooccur_mat.rows, cooccur_mat.data)):\n",
    "            for data_idx, j in enumerate(row):\n",
    "                yield i, j, data[data_idx]\n",
    "    \n",
    "    batches = list(single_gen())\n",
    "                \n",
    "    while True:\n",
    "        np.random.shuffle(batches)\n",
    "        for batch in range(0, len(batches), batch_size):\n",
    "            yield zip(*batches[batch:batch+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooccurrence_cap = 250\n",
    "scaling_factor_c = 0.75\n",
    "batch_size = 512\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove_graph = tf.Graph()\n",
    "\n",
    "with glove_graph.as_default():\n",
    "    count_max      = tf.constant([cooccurrence_cap], dtype=tf.float32, name='max_cooccurrence_count')\n",
    "    scaling_factor = tf.constant([scaling_factor_c], dtype=tf.float32, name=\"scaling_factor\")\n",
    "\n",
    "    focal_input        = tf.placeholder(tf.int32,   shape=[batch_size], name=\"focal_words\")\n",
    "    context_input      = tf.placeholder(tf.int32,   shape=[batch_size], name=\"context_words\")\n",
    "    cooccurrence_count = tf.placeholder(tf.float32, shape=[batch_size], name=\"cooccurrence_count\")\n",
    "\n",
    "    focal_embeddings = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            [wiki.VOCAB_SIZE, embedding_dim],\n",
    "            1.0,\n",
    "            -1.0\n",
    "        ),\n",
    "        name=\"focal_embeddings\"\n",
    "    )\n",
    "    context_embeddings = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            [wiki.VOCAB_SIZE, embedding_dim],\n",
    "            1.0,\n",
    "            -1.0\n",
    "        ),\n",
    "        name=\"context_embeddings\"\n",
    "    )\n",
    "\n",
    "    focal_biases   = tf.Variable(tf.random_uniform([wiki.VOCAB_SIZE], 1.0, -1.0), name='focal_biases')\n",
    "    context_biases = tf.Variable(tf.random_uniform([wiki.VOCAB_SIZE], 1.0, -1.0), name=\"context_biases\")\n",
    "\n",
    "    focal_embedding   = tf.nn.embedding_lookup([focal_embeddings],   focal_input)\n",
    "    context_embedding = tf.nn.embedding_lookup([context_embeddings], context_input)\n",
    "    focal_bias        = tf.nn.embedding_lookup([focal_biases],       focal_input)\n",
    "    context_bias      = tf.nn.embedding_lookup([context_biases],     context_input)\n",
    "\n",
    "    weighting_factor = tf.minimum(\n",
    "        1.0,\n",
    "        tf.pow(\n",
    "            tf.div(\n",
    "                cooccurrence_count,\n",
    "                count_max),\n",
    "            scaling_factor\n",
    "        )\n",
    "    )\n",
    "\n",
    "    embedding_product = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)\n",
    "\n",
    "    log_cooccurrences = tf.log(tf.to_float(cooccurrence_count))\n",
    "\n",
    "    distance_expr = tf.square(tf.add_n([\n",
    "        embedding_product,\n",
    "        focal_bias,\n",
    "        context_bias,\n",
    "        tf.negative(log_cooccurrences)]\n",
    "    ))\n",
    "\n",
    "    glove_losses = tf.multiply(weighting_factor, distance_expr)\n",
    "    glove_loss = tf.reduce_sum(glove_losses)\n",
    "    optimizer = tf.train.AdagradOptimizer(0.5).minimize(glove_loss)\n",
    "\n",
    "    combined_embeddings = tf.add(focal_embeddings, context_embeddings, name=\"combined_embeddings\")\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_glove(wiki, num_steps = 750):\n",
    "    average_loss = 0\n",
    "\n",
    "    with tf.Session(graph=glove_graph) as session:\n",
    "        init.run()\n",
    "        for step in range(num_steps):\n",
    "            try:\n",
    "                batch_gen = glove_batch_gen(cooccurrences_matrix, batch_size)\n",
    "                for _ in range(1000):\n",
    "                    centers, contexts, counts = next(batch_gen)\n",
    "                    if len(counts) != batch_size:\n",
    "                        continue\n",
    "                    feed_dict = {\n",
    "                        focal_input: centers,\n",
    "                        context_input: contexts,\n",
    "                        cooccurrence_count: counts\n",
    "                    }\n",
    "                    _, loss_val = session.run([optimizer, glove_loss], feed_dict=feed_dict)\n",
    "                    average_loss += loss_val\n",
    "                clear_output()\n",
    "                print(\n",
    "                    \"Time: {}; steps: {}; avg. loss: {}.\".format(\n",
    "                        datetime.now().strftime(\"%H:%M:%S\"),\n",
    "                        step*1000,\n",
    "                        average_loss/1000)\n",
    "                )\n",
    "                average_loss = 0\n",
    "                glove_embeds = combined_embeddings.eval()\n",
    "                for i in range(10):\n",
    "                    w = wiki.rev_vocab[random.randint(1, 150)]\n",
    "                    print(w.ljust(10), \":\", \" \".join(closest_words(w, glove_embeds, wiki)[1:]))\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "        glove_embeds = combined_embeddings.eval()\n",
    "    return glove_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_glove(wiki):\n",
    "    if not os.path.exists(wiki.GLOVE_EMBED_FILE):\n",
    "        glove_embeds = build_glove(wiki)\n",
    "        np.save(wiki.GLOVE_EMBED_FILE, glove_embeds)\n",
    "    else:\n",
    "        glove_embeds = np.load(wiki.GLOVE_EMBED_FILE)\n",
    "    return glove_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_embeds = get_glove(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Теперь T-SNE. Тут тоже просто библиотечная реализация. Единственное, если вогнать всю матрицу, падает от нехватки памяти, так что обойдёмся первыми 10к векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(3)\n",
    "post_tsne_glove = tsne.fit(glove_embeds[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(10,10)) \n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "for i, (label, _) in enumerate(collections.Counter(wiki.counter).most_common(100)):\n",
    "    x, y, z = post_tsne_glove.embedding_[i]\n",
    "    ax.scatter(x, y, z, s=9, alpha=0.1)\n",
    "    ax.text(x, y, z, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Не вполне ясно, что предлагается из этого выжимать, впрочем. Давайте просто померяем углы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def angle(wiki, s1, w1, s2, w2):\n",
    "    return np.arccos(cosine_similarity(\n",
    "        [sum(post_tsne_glove.embedding_[wiki.vocab[x]]*sign for sign, x in zip(s1, w1))],\n",
    "        [sum(post_tsne_glove.embedding_[wiki.vocab[x]]*sign for sign, x in zip(s2, w2))],\n",
    "    ))[0][0]*90/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "angle(wiki, [1, -1], [\"man\", \"woman\"], [1, -1], [\"mr\", \"ms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Более-менее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "angle(wiki, [1, -1], [\"good\", \"best\"], [1, -1], [\"bad\", \"worst\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Метрика для сравнения моделей --- количество правильных ответов на вопросы из датасета Миколова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"questions-words.txt\") as question_file:\n",
    "    parsed = (\n",
    "        line for line in (\n",
    "            tuple(wiki._normalize(line)) for line in question_file.readlines() if not line.startswith(\":\")\n",
    "        ) if len(line) == 4\n",
    "    )\n",
    "\n",
    "    *questions, labels = zip(*parsed)\n",
    "\n",
    "    questions = list(zip(*questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate(embeds, wiki, questions, labels):\n",
    "    total = 0\n",
    "    for i, (label, (pos1, neg, pos2)) in enumerate(zip(labels, questions)):\n",
    "        if i % 50 == 0:\n",
    "            print(\"{}\\r\".format((i, total)), end=\"\")\n",
    "        total += (label in set(x[1] for x in analogy([pos1, pos2], [neg], embeds, wiki)))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(lsa_embeds, wiki, questions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "evaluate(w2v_embeds, wiki, questions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "evaluate(glove_embeds, wiki, questions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ну. Могло быть лучше, конечно, но w2v всех обошёл в ~2 раза"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
