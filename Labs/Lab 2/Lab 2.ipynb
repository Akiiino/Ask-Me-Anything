{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Вычислите градиент функции $Q$ (попробуйте провести выкладки для отдельной строки $w_i$).\n",
    "2. Обучите модель с помощью градиентного спуска на выборке [mnist](https://www.kaggle.com/c/digit-recognizer) (вы можете применить свою любимую вариацию метода).\n",
    "3. Вычислите качество на отоженной выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Вычислим градиент:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала перепишем формулу $Q$ так:\n",
    "\n",
    "$$\n",
    "  Q(W) =\n",
    "  -\\frac{1}{\\mathcal{l}}\\sum_y\\sum_i [y = i] \\cdot \\ln(p_i(W)) =\n",
    "  -\\frac{1}{\\mathcal{l}}\\sum\\limits_{x, y}\\sum_i [y = i] \\cdot \\ln(\\mathrm{softmax}(Wx)_i)\n",
    "$$\n",
    "\n",
    "Заметим, что сумма по $i$ состоит из нулей и единственного ненулевого слагаемого --- того, где $i = y$. Значит, мы можем упростить выражение:\n",
    "\n",
    "$$\n",
    "  Q(W) =\n",
    "  -\\frac{1}{\\mathcal{l}}\\sum\\limits_{x, y} \\cdot \\ln(\\mathrm{softmax}(Wx)_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем найти градиент. Т.к. мы ищем градиент функции, переводящей матрицу в скаляр, градиент представляет из себя матрицу той же размерности, что и $W$. Найдём её $ij$-ый элемент:\n",
    "\n",
    "$$\n",
    "  \\left(\\frac{dQ}{dW}\\right)_{ij} =\n",
    "  \\frac{dQ}{dW_{ij}} =\n",
    "  \\frac{dQ}{dw_i} \\cdot\n",
    "  \\frac{dw_i}{dW_{ij}}\n",
    "$$\n",
    "\n",
    "Начнём с простого и поймём, что из себя представляет $\\frac{dw_i}{dW_{ij}}$. Для простоты посчитаем по определению:\n",
    "\n",
    "$$\n",
    "  \\lim\\limits_{\\Delta W_{ij} \\to 0} \\frac{(W_{i1}, \\ldots, W_{ij}, \\ldots, W_{in}) - (W_{i1}, \\ldots, W_{ij}+\\Delta W_{ij}, \\ldots, W_{in})}{\\Delta W_{ij}} =\n",
    "  \\lim\\limits_{\\Delta W_{ij} \\to 0} \\frac{(0, \\Delta W_{ij}, \\ldots, 0)}{\\Delta W_{ij}} = (\\underbrace\n",
    "  {0, \\ldots,}_{j-1} 1, \\ldots, 0)\n",
    "$$\n",
    "\n",
    "Отлично. Теперь заметим, что так как $ij$-ый элемент матрицы градиента --- лишь $j$-ый элемент производной по $i$-ой строке, можно просто считать градиент построчно, т.е.\n",
    "\n",
    "$$\n",
    "  \\frac{dQ}{dW} =\n",
    "  \\begin{pmatrix}\n",
    "    \\frac{dQ}{dw_1}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\frac{dQ}{dw_k}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\frac{dQ}{dw_n}\\\\\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Найдём производную по строке:\n",
    "\n",
    "$$\n",
    "  \\frac{dQ}{dw_k} =\n",
    "  -\\frac{1}{l}\\sum\\limits_i\\frac{1}{\\mathrm{softmax}(Wx^i)_{y_i}}\\cdot\\frac{d\\mathrm{softmax}(Wx^i)_{y_i}}{dw_k} =\n",
    "  \\begin{cases}\n",
    "    \\frac{d}{dw_k} \\frac{e^{w_yx^k}}{\\sum\\limits_j e^{w_jx^j}},&k \\neq y\\\\\n",
    "    \\frac{d}{dw_k} \\frac{e^{w_kx^k}}{\\sum\\limits_j e^{w_jx^j}},&k = y\n",
    "  \\end{cases} =\n",
    "  \\begin{cases}\n",
    "    e^{w_yx^k}\n",
    "    \\frac{d}{dw_k} \\frac{1}{\\sum\\limits_j e^{w_jx^j}},&k \\neq y\\\\\n",
    "    \\frac{x^k e^{w_kx^k}\\sum e^{w_jx^j} - e^{w_kx^k} x^ke^{w_kx^k}}{\\left(\\sum e^{w_jx^j}\\right)^2},&k = y\n",
    "  \\end{cases} =\n",
    "  \\begin{cases}\n",
    "    e^{w_yx^k}\n",
    "    \\frac{-1}{\\left(\\sum e^{w_jx^j}\\right)^2}\\cdot\\frac{d}{dw_k}\\sum e^{w_jx^j},&k \\neq y\\\\\n",
    "    \\frac{x^k e^{w_kx^k}\\left(\\sum e^{w_jx^j} - e^{w_kx^k} \\right)}{\\left(\\sum e^{w_jx^j}\\right)^2},&k = y\n",
    "  \\end{cases} =\n",
    "  \\begin{cases}\n",
    "    e^{w_yx^k}\n",
    "    \\frac{-1}{\\left(\\sum e^{w_jx^j}\\right)^2}\\cdot x^ke^{w_kx^k},&k \\neq y\\\\\n",
    "    x^k \\frac{e^{w_kx^k}}{\\sum e^{w_jx^j}}\\cdot\\left(1-\\frac{e^{w_kx^k}}{\\sum e^{w_jx^j}}\\right),&k = y\n",
    "  \\end{cases} =\n",
    "  \\begin{cases}\n",
    "    -x^k\\frac{e^{w_yx^k}}{\\sum e^{w_jx^j}}\\cdot\\frac{e^{w_kx^k}}{\\sum e^{w_jx^j}},&k \\neq y\\\\\n",
    "    -x^k \\frac{e^{w_kx^k}}{\\sum e^{w_jx^j}}\\cdot\\left(\\frac{e^{w_kx^k}}{\\sum e^{w_jx^j}} - 1\\right),&k = y\n",
    "  \\end{cases} =\n",
    "  \\begin{cases}\n",
    "    -x^k \\frac{e^{w_yx^k}}{\\sum e^{w_jx^j}}\\cdot\\frac{e^{w_kx^k}}{\\sum e^{w_jx^j}},&k \\neq y\\\\\n",
    "    -x^k \\frac{e^{w_yx^k}}{\\sum e^{w_jx^j}}\\cdot\\left(\\frac{e^{w_kx^k}}{\\sum e^{w_jx^j}} - 1\\right),&k = y\n",
    "  \\end{cases} = -x^k \\frac{e^{w_yx^k}}{\\sum e^{w_jx^j}}\\cdot\\left(\\frac{e^{w_kx^k}}{\\sum e^{w_jx^j}} - \\delta_{ky}\\right)\n",
    "$$\n",
    "\n",
    "Здесь $\\delta$ --- символ Кронекера, принимающий значение 1 при равенстве индексов. Упростим выражение ещё сильнее:\n",
    "\n",
    "Введём матрицу $\\mathrm{softmax}(XW) = \\begin{pmatrix}\\mathrm{softmax}(x_1W)\\\\\\vdots\\\\\\mathrm{softmax}(x_iW)\\\\\\vdots\\\\\\mathrm{softmax}(x_nW)\\end{pmatrix}$ и матрицу $\\delta_y = (\\delta_{y_ij})_{ij}$, т.е. столбец строк, где единица стоит только на месте, соответствующем номеру класса. Тогда всё выражение упрощается до\n",
    "\n",
    "$$\n",
    "  \\frac{dQ}{dW} = \\frac{1}{l}X^T(\\mathrm{softmax}(XW) - \\delta_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2. Обучим сеть:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала все базовые приготовления:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "digits = X_train.reshape((60000, 784))\n",
    "test_digits = X_test.reshape((10000, 784))\n",
    "digits = digits.astype('float32')\n",
    "test_digits = test_digits.astype('float32')\n",
    "digits /= 255\n",
    "test_digits /= 255\n",
    "classes_num = 10\n",
    "\n",
    "labels = to_categorical(y_train, classes_num)\n",
    "test_labels = to_categorical(y_test, classes_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся Adam'ом из прошлой лабораторной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam(grad, points, labels, learning_rate, decay_first, decay_second, batch_size, passes, start_point):\n",
    "    first_moment = np.zeros((points.shape[1], 10))\n",
    "    second_moment = np.zeros((points.shape[1], 10))\n",
    "    curr_point = start_point\n",
    "    interm_sequence = np.array(start_point)\n",
    "\n",
    "    batch_num = (points.shape[0]-1) // batch_size + 1\n",
    "    \n",
    "    step = 0\n",
    "    for i in range(passes):\n",
    "        for j in range(batch_num):\n",
    "            step += 1\n",
    "\n",
    "            curr_grad = grad(curr_point, j, batch_size)\n",
    "            first_moment = decay_first*first_moment + (1-decay_first)*curr_grad\n",
    "            bias_corr_first = first_moment/(1-decay_first**step)\n",
    "            \n",
    "            second_moment = decay_second*second_moment + (1-decay_second)*curr_grad**2       \n",
    "            bias_corr_second = second_moment/(1-decay_second**step)\n",
    "\n",
    "            curr_point -= learning_rate*bias_corr_first/(np.sqrt(bias_corr_second)+1e-8)\n",
    "\n",
    "    return curr_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем вычисление градиента и прочие полезные вещи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MNIST_Q(curr_point):\n",
    "    return -1/digits.shape[0]*sum(np.log(softmax(x @ curr_point)[y]+1e-10) for x, y in zip(digits, raw_labels))\n",
    "\n",
    "def MNIST_grad(curr_point, batch_num, batch_size):\n",
    "    curr_slice = slice(batch_num*batch_size, min((batch_num+1)*batch_size, digits.shape[0]))\n",
    "    raw = digits[curr_slice] @ curr_point\n",
    "    return 1/batch_size * digits[curr_slice].T @ (np.vstack([softmax(raw[i]) for i in range(raw.shape[0])]) - labels[curr_slice])\n",
    "\n",
    "def predict(matrix, digit):\n",
    "    raw = digit @ matrix;\n",
    "    return softmax(raw)\n",
    "\n",
    "def accuracy_check(classifier, test_points, test_labels):\n",
    "    correct_count = 0\n",
    "    for i in range(test_digits.shape[0]):\n",
    "        guess = np.argmax(predict(classifier, test_points[i]))\n",
    "        correct_count += (guess == np.argmax(test_labels[i]))\n",
    "    return correct_count/test_points.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно и обучать. Но обучать наугад неинтересно и неэффективно, поэтому попробуем найти оптимальное значение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimal_learn_rate = (0, 0)\n",
    "for i in np.arange(-8, 1, 0.3):\n",
    "    print(i, end=\"... \")\n",
    "    classifier = adam(\n",
    "        MNIST_grad,\n",
    "        digits,\n",
    "        labels,\n",
    "        10**i,\n",
    "        0.9,\n",
    "        0.999,\n",
    "        501,\n",
    "        10,\n",
    "        np.zeros((digits.shape[1], 10))\n",
    "    )\n",
    "    accuracy = accuracy_check(classifier, test_digits, test_labels)\n",
    "    if accuracy > optimal_learn_rate[1]:\n",
    "        optimal_learn_rate = (i, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = adam(\n",
    "    MNIST_grad,\n",
    "    digits,\n",
    "    labels,\n",
    "    10**optimal_learn_rate[0],\n",
    "    0.9,\n",
    "    0.999,\n",
    "    500,\n",
    "    10,\n",
    "    np.zeros((digits.shape[1], 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим точность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy_check(classifier, test_digits, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92% --- довольно неплохо для простой однослойной сети!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Как стоит подбирать значения $\\lambda_1$ и $\\lambda_2$?\n",
    "2. Удалось ли улучшить $Q$ на отложенной выборке?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала реализуем градиент регуляризованной функции ошибки. Он довольно простой, т.к. функции базовые:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MNIST_grad_reg(curr_point, batch_num, batch_size, l_1, l_2):\n",
    "    curr_slice = slice(batch_num*batch_size, min((batch_num+1)*batch_size, digits.shape[0]))\n",
    "    raw = digits[curr_slice] @ curr_point \n",
    "    return (\n",
    "        1/batch_size * digits[curr_slice].T @ (np.vstack([softmax(raw[i]) for i in range(raw.shape[0])]) - labels[curr_slice]) +\n",
    "        l_1*np.sign(curr_point) +\n",
    "        l_2*2*(curr_point)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И снова попробуем найти хорошие коэффициенты. Давайте теперь для красоты построим график:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_range = np.arange(-8, 1, 0.3)\n",
    "accuracies = np.zeros((lambda_range.shape[0], lambda_range.shape[0]))\n",
    "losses = np.zeros((lambda_range.shape[0], lambda_range.shape[0]))\n",
    "\n",
    "for l_1 in range(lambda_range.shape[0]):\n",
    "    for l_2 in range(lambda_range.shape[0]):\n",
    "        classifier = adam(\n",
    "            lambda p, n, s: (MNIST_grad_reg(p, n, s, 10**lambda_range[l_1], 10**lambda_range[l_2])),\n",
    "            digits,\n",
    "            labels,\n",
    "            10**optimal_learn_rate[0],\n",
    "            0.9,\n",
    "            0.999,\n",
    "            501,\n",
    "            10,\n",
    "            np.zeros((digits.shape[1], 10))\n",
    "        )\n",
    "        accuracies[l_1, l_2] = accuracy_check(classifier, test_digits, test_labels)\n",
    "        losses[l_1, l_2] = MNIST_Q(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib notebook\n",
    "\n",
    "reg_figure = plt.figure()\n",
    "reg_plot = reg_figure.gca(projection='3d')\n",
    "\n",
    "X = np.arange(-3, 3, 0.15)\n",
    "Y = np.arange(-10, 10, 0.15)\n",
    "reg_plot.view_init(90, 0)\n",
    "\n",
    "reg_plot.plot_surface(\n",
    "    *np.meshgrid(lambda_range,\n",
    "    lambda_range),\n",
    "    accuracies,\n",
    "    rstride=1,\n",
    "    cstride=1,\n",
    "    cmap=cm.get_cmap('jet', 200),\n",
    "    norm=matplotlib.colors.LogNorm(),\n",
    "    linewidth=0,\n",
    "    antialiased=True\n",
    ")\n",
    "\n",
    "reg_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В общем, видно, что оптимально получается при минимальном влиянии регуляризаторов. Видимо, под нашу задачу они не подходят, что тут поделать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Предложите значения $w$ и $b$, чтобы $y$ реализовала операторы *and*, *or*, *not*.\n",
    "2. Приведите пример булевой функции, которая не может быть представлена в виде $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Реализуемые функции:\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item $\\texttt{and}$:\n",
    "  $w = (1, \\ldots, 1),\\ b = -n+0.5$\n",
    "  \n",
    "  $$\\theta(wx+b) = \\theta\\left(\\sum x_i - n + 0.5\\right)$$\n",
    "  \n",
    "  Очевидно, что только при $x = (1, \\ldots, 1)$ $\\sum x_i + 0.5 > n$.\n",
    "\\item $\\texttt{or}$:\n",
    "  $w = (1, \\ldots, 1),\\ b = -0.5$\n",
    "  \n",
    "  $$\\theta(wx+b) = \\theta\\left(\\sum x_i - 0.5\\right)$$\n",
    "  \n",
    "  Очевидно, что только при $x = (0, \\ldots, 0)$ $\\sum x_i - 0.5 < 0$.\n",
    "\\item $\\texttt{not}$:\n",
    "  $w = -E ,\\ b = 0.5E$\n",
    "  \n",
    "  $$\\theta(wx+b) = \\theta\\left(\\begin{pmatrix}0.5 - x_1\\\\\\vdots\\\\0.5 - x_i\\\\\\vdots\\\\0.5 - x_n\\end{pmatrix}\\right)$$\n",
    "  \n",
    "  Очевидно, что $0.5-x = \\overline{x}$.\n",
    "\n",
    "\\end{itemize}\n",
    "\n",
    "(я только сейчас понял, что случайно решил для общего случая, но пусть будет)\n",
    "\n",
    "##### Нереализуемые функции:\n",
    "\n",
    "Например, $\\texttt{xor}$. Ясно, что $wx+b$ даёт нам некоторое линейное преобразование; из линейности преобразования $f$ следует, что $f\\left(\\frac{a+b}{2}\\right) = \\frac{f(a) + f(b)}{2}$. Однако:\n",
    "\n",
    "\\begin{itemize}\n",
    "  \\item\n",
    "  $\n",
    "    f\\left(\\begin{pmatrix}0.5\\\\0.5\\end{pmatrix}\\right) =\n",
    "    f\\left(\\frac{\\begin{pmatrix}0\\\\0\\end{pmatrix}+\\begin{pmatrix}1\\\\1\\end{pmatrix}}{2}\\right) =\n",
    "    \\frac{\n",
    "     \\overbrace{f\\left(\\begin{pmatrix}0\\\\0\\end{pmatrix}\\right)}^{< 0.5} \n",
    "    +\\overbrace{f\\left(\\begin{pmatrix}1\\\\1\\end{pmatrix}\\right)}^{< 0.5}}{2} < \\frac{1}{2}\n",
    "  $\n",
    "  \\item\n",
    "  $\n",
    "    f\\left(\\begin{pmatrix}0.5\\\\0.5\\end{pmatrix}\\right) =\n",
    "    f\\left(\\frac{\\begin{pmatrix}1\\\\0\\end{pmatrix}+\\begin{pmatrix}0\\\\1\\end{pmatrix}}{2}\\right) =\n",
    "    \\frac{\n",
    "     \\overbrace{f\\left(\\begin{pmatrix}1\\\\0\\end{pmatrix}\\right)}^{> 0.5} \n",
    "    +\\overbrace{f\\left(\\begin{pmatrix}0\\\\1\\end{pmatrix}\\right)}^{> 0.5}}{2} > \\frac{1}{2}\n",
    "  $\n",
    "\\end{itemize}\n",
    "\n",
    "Противоречие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Можете ли вы теперь представить вашу функцию в виде $y$?\n",
    "2. Может ли $y$ реализовать произвольную булеву функцию? Знаете ли вы, что такое ДНФ?\n",
    "\n",
    "Давайте сразу решать общий случай:\n",
    "\n",
    "Да, может. Вспомним (вернее, ещё не забыли), что мы можем сделать \"И\", \"ИЛИ\" и \"НЕ\" с помощью линейных преобразований; заметим также, что линейная комбинация операций \"НЕ\" и \"И\" тоже линейна; значит, мы можем с помощью внутреннего преобразования \"создать\" конъюнкты, а помощью внешнего соединить их дизъюнкциями. Получим ДНФ, а значит, и любую бинарную функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь поработаем без палок и верёвок; используем умный и удобный Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "2. Реализуйте двухслойную сеть, где в качестве нелинейной функции используется [ReLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks).\n",
    "3. Улучшилось ли качество в сравнении с предыдущей моделью?\n",
    "4. Какова размерность выходного вектора после первого линейного преобразования? Как влияет его размер? Постройте график.\n",
    "5. Предложите свою архитектуру\n",
    "6. Как зависит качество от количества слоев? Постройте график.\n",
    "7. Попробуйте постепенно добавлять слои в вашу сеть, для это изучите следующие трюки:\n",
    " * [Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)\n",
    " * [RBM](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.noise import GaussianDropout, GaussianNoise\n",
    "from keras.optimizers import Adam, Adadelta, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не будем изобретать велосипед и воспользуемся шаблоном из примера от Keras, изменив его под наши нужды. Нам всего-то нужно два Dense слоя с перемежающими их активациями --- ReLu в середине и softmax на выходе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epoch_number = 10\n",
    "\n",
    "layers = [\n",
    "    Dense(512, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(classes_num),\n",
    "    Activation('softmax'),\n",
    "]\n",
    "\n",
    "simple_model = Sequential(layers)\n",
    "\n",
    "simple_model.summary()\n",
    "\n",
    "simple_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = simple_model.fit(\n",
    "    digits,\n",
    "    labels,\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch=epoch_number,\n",
    "    verbose=0,\n",
    "    validation_data=(test_digits, test_labels)\n",
    ")\n",
    "\n",
    "score = simple_model.evaluate(test_digits, test_labels, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даже такая довольно нехитрая сеть обходит по точности нашу рукописную, а ведь она ещё и реализуется проще. А что вообще можно выжать из такой простой архитектуры? Можно варьировать размер скрытого слоя, например, или размер батча. Интуитивно, впрочем, кажется, что при большинстве размеров батча ничего принципиально не должно меняться --- разве что, на очень малых размерах будет гораздо медленнее считаться. А вот размер слоя может оказывать большее влияние. Попробуем построить график точности от размера скрытого слоя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "classes_number = 10\n",
    "epoch_number = 10\n",
    "max_size = 10\n",
    "\n",
    "hidd_losses = np.zeros((max_size))\n",
    "hidd_accuracies = np.zeros((max_size))\n",
    "\n",
    "for i in range(0, max_size):\n",
    "    print(i, end=\"... \")\n",
    "    layers = [\n",
    "        Dense(2**i, input_shape=(784,)),\n",
    "        Activation('relu'),\n",
    "        Dense(classes_number),\n",
    "        Activation('softmax'),\n",
    "    ]\n",
    "\n",
    "    comp_model = Sequential(layers)\n",
    "\n",
    "    comp_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = comp_model.fit(\n",
    "        digits,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        nb_epoch=epoch_number*(i+1),\n",
    "        verbose=0,\n",
    "        validation_data=(test_digits, test_labels)\n",
    "    )\n",
    "\n",
    "    score = comp_model.evaluate(test_digits, test_labels, verbose=0)\n",
    "    hidd_losses[i] = score[0]\n",
    "    hidd_accuracies[i] = score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size_figure = plt.figure()\n",
    "size_plot = size_figure.add_subplot(111)\n",
    "size_plot.set_xlabel(\"log n\")\n",
    "size_plot.set_ylabel(\"accuracy\")\n",
    "\n",
    "X = np.arange(0, max_size)\n",
    "\n",
    "size_plot.plot(\n",
    "    X,\n",
    "    hidd_accuracies,\n",
    "    linewidth=1,\n",
    "    antialiased=True,\n",
    "    label=\"Accuracy\"\n",
    ")\n",
    "size_plot.plot(\n",
    "    X,\n",
    "    hidd_losses,\n",
    "    linewidth=1,\n",
    "    antialiased=True,\n",
    "    label=\"Loss\"\n",
    ")\n",
    "\n",
    "size_plot.legend(loc=\"upper center\")\n",
    "\n",
    "size_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что прирост сильно замедляется с ростом размерности скрытого слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь придумаем чуть более сложную архитектуру и пообучаем её подольше; это должно позволить нам добиться ещё большей точноcти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "nb_classes = 10\n",
    "nb_epoch = 150\n",
    "\n",
    "initial_layers = [\n",
    "    Dense(512, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512),\n",
    "    Activation('relu'),\n",
    "    BatchNormalization(),\n",
    "]\n",
    "\n",
    "out = [\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "]\n",
    "\n",
    "model = Sequential(initial_layers + out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    digits,\n",
    "    labels,\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch=nb_epoch,\n",
    "    verbose=2,\n",
    "    validation_data=(test_digits, test_labels)\n",
    ")\n",
    "\n",
    "score = model.evaluate(test_digits, test_labels, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$98.8\\%$! Неплохо; обучается небыстро, правда."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "Мы уже смотрели, как зависит качество от размера скрытого слоя; а давайте теперь поменяем число самих слоёв?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "classes_number = 10\n",
    "epoch_number = 5\n",
    "max_size = 10\n",
    "\n",
    "hidd_num_losses = np.zeros((max_size))\n",
    "hidd_num_accuracies = np.zeros((max_size))\n",
    "\n",
    "for i in range(0, max_size):\n",
    "    print(i, end=\"... \")\n",
    "    layers = [\n",
    "        Dense(512, input_shape=(784,)),\n",
    "        Activation('relu')\n",
    "    ]\n",
    "    \n",
    "    for _ in range(i):\n",
    "        layers += [\n",
    "            Dense(512),\n",
    "            Activation('relu')\n",
    "        ]\n",
    "    \n",
    "    layers += [\n",
    "        Dense(classes_number),\n",
    "        Activation('softmax'),\n",
    "    ]\n",
    "\n",
    "    comp_model = Sequential(layers)\n",
    "\n",
    "    comp_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = comp_model.fit(\n",
    "        digits,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        nb_epoch=epoch_number*(i+1),\n",
    "        verbose=2,\n",
    "        validation_data=(test_digits, test_labels)\n",
    "    )\n",
    "\n",
    "    score = comp_model.evaluate(test_digits, test_labels, verbose=0)\n",
    "    hidd_num_losses[i] = score[0]\n",
    "    hidd_num_accuracies[i] = score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоследок поэкспериментируем с автоэнкодерами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise(orig, model, n = 10):\n",
    "    decoded = model.predict(orig[:n])\n",
    "    draw = plt.figure(figsize=(10, 2))\n",
    "    for i in range(n):\n",
    "        ax = draw.add_subplot(2, n, i + 1)\n",
    "        ax.imshow(orig[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = draw.add_subplot(2, n, i + 1 + n)\n",
    "        ax.imshow(decoded[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    draw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "nb_epoch = 100\n",
    "\n",
    "coding_layers = [\n",
    "    Dense(1024, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(128),\n",
    "    Activation('relu')\n",
    "]\n",
    "\n",
    "decoding_layers = [\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dense(1024),\n",
    "    Activation('relu'),\n",
    "    Dense(784),\n",
    "    Activation('relu')\n",
    "]\n",
    "\n",
    "model = Sequential(coding_layers + decoding_layers)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adadelta(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    digits,\n",
    "    digits,\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch=nb_epoch,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualise(digits, model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "А теперь самое интересное --- ведь автоэнкодер научился извлекать какие-то фичи; почему бы не основать на этом ещё одну сеть?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in coding_layers:\n",
    "    l.trainable = False\n",
    "\n",
    "batch_size = 1024\n",
    "nb_epoch = 200\n",
    "\n",
    "processing_layers = [\n",
    "    Dense(512),\n",
    "    Activation('relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(classes_num),\n",
    "    Activation('softmax')\n",
    "]\n",
    "\n",
    "model = Sequential(coding_layers+processing_layers)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    digits,\n",
    "    labels,\n",
    "    batch_size=batch_size,\n",
    "    nb_epoch=nb_epoch,\n",
    "    verbose=2,\n",
    "    validation_data=(test_digits, test_labels)\n",
    ")\n",
    "\n",
    "score = model.evaluate(test_digits, test_labels, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
